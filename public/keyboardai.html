<!DOCTYPE html>
<html lang="en">
        <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>Keyboard_AI</title>
                <link rel="stylesheet" href="../styles.css"/>

        </head>


<body>
<div class="placeholder">
        <br/>

        <!--dark-mode-button-->
        <div class ="buttonSytle">
                <button onclick ="lightmodeFun()" id="uniqueButton" type="button">dark|light</button>
        </div>
</div>
<br/><br/>


<div class="projects">
        <a style="font-size:16px;"href="../public/projects.html">back</a>
</div>
<br/>

<br/>
<div class="microscope">
        <h2 id="mic">keyboard_ai</h2>
	<p> This is a Neovim extension that implements an interface for LOCAL code generation. <br/> I'm actually not too bullish on using AI to create code since I find offloading problem solving and thinking hugely problematic and not how I like to utilize LLMs. While writing boiler plate code might be a legacy problem, I don't think this is where the deep value of programming lies. <br/> But this is interesting nonetheless and could serve as a blueprint just for interacting with buffers for future projects.</p>
  <p> I'm using a custom model-file that outputs JSON in a custom template which allows the LLM to have control over deletion and insertion as those are the atomic operations we need to replace code.</p>
	<p> This uses Neovim because of the good extensibility with LUA. A config file in LUA is required to be able to use the custom commands in NVIM. A LUA script handles the reading of the buffer and highlighting changes. Instead of having a dif function I simply pass the linechanges, this requires less compute. </p>
  <p> The python script initiates the LLM and changes are made. To avoid more complexity by inter-process communication between LUA and Python I simply store the changes in temporary files, might integrate version control later. In theory this is not good practice because disk access is slower than RAM reads/writes but for now this is the least of this programs problems.</p>
  <br/>
  <p> The issue here is the model anyways, which is why in the long-term having the only moat for some companies in the models owned by Google and OpenAI is problematic. Once they decide to integrate a coding editor into their web-app (at lower cost) these wrapper apps (while exciting) are probably going away.</p>
  <p> Now what would be interesting for the future might be to optimize smaller models to be able to generate USEFUL code. Generally I am optimistic about the prospects of local compute and to remain digitally sovereign.</p>
  <br/>
	<a href="https://github.com/seve05/Keyboard_AI">Keyboard_AI</a>
	<br/>
</div>
<br/>
<div id="image-container">
	<img src" alt="..">
</div>

<br/><br/>
<p id="sent" style="width:70%; margin-left:25%;" class="form_field"></p>
<img id="gendImg" class="form_field" >
<!-- Server side generated image in here UPDATE: ill rather provide a download 
because of the security implications as of right now it would be hosted locally-->
<!--<img class="form_field" id="generatedImage" src="">-->
<br/><br/>
<hr id = "prohr"/>
<br/>
<footer id ="foot">Â© 2025 Severin Richter</footer>
</body>





<!-- dark(light) mode script in local html-->
<script>
function lightmodeFun() {
   var element = document.body;
   element.classList.toggle("light-mode");
        }
</script>


</html>
